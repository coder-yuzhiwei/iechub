{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import tensorflow as tf\n",
    "from bert_modified import modeling\n",
    "import numpy as np\n",
    "from bert_modified import tokenization\n",
    "import tensorflow.contrib.keras as kr\n",
    "import json\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "__author__ = 'Yuzhong Hong <hongyuzhong@qiyi.com>'\n",
    "__date__, __version__ = '02/26/2019', '0.1'  # Module Creation.\n",
    "__date__, __version__ = '04/04/2019', '0.2'  # Add on-demand n-gram masked language model\n",
    "\n",
    "\n",
    "__description__ = 'Masked language model'\n",
    "\n",
    "__future_work__ = '1. improve computational efficiency by changing scalar computation to matrix computation'\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# BIGRAMS = pickle.load(open('bigram_dict_simplified.sav', 'rb'))\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    max_seq_length = 16\n",
    "    vocab_file = \"model/pre-trained/vocab.txt\"\n",
    "    bert_config_file = \"model/pre-trained/bert_config.json\"\n",
    "    init_checkpoint = \"model/pre-trained/bert_model.ckpt\"\n",
    "    bert_config = modeling.BertConfig.from_json_file(bert_config_file)\n",
    "    topn = 5\n",
    "    bigrams = None  # pickle.load(open('bigram_dict_simplified.sav', 'rb'))\n",
    "\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        # placeholders\n",
    "        self.input_ids = tf.placeholder(tf.int32, [None, self.config.max_seq_length], name='input_ids')\n",
    "        self.input_mask = tf.placeholder(tf.int32, [None, self.config.max_seq_length], name='input_mask')\n",
    "        self.segment_ids = tf.placeholder(tf.int32, [None, config.max_seq_length], name='segment_ids')\n",
    "        self.masked_lm_positions = tf.placeholder(tf.int32, [None, self.config.max_seq_length - 2],\n",
    "                                                  name='masked_lm_positions')\n",
    "        self.masked_lm_ids = tf.placeholder(tf.int32, [None, self.config.max_seq_length - 2],\n",
    "                                            name='masked_lm_ids')\n",
    "        self.masked_lm_weights = tf.placeholder(tf.float32, [None, self.config.max_seq_length - 2],\n",
    "                                                name='masked_lm_weights')\n",
    "\n",
    "        is_training = False\n",
    "\n",
    "        # create model\n",
    "        masked_lm_loss, masked_lm_example_loss, self.masked_lm_log_probs, self.probs = self.create_model(\n",
    "            self.input_ids,\n",
    "            self.input_mask,\n",
    "            self.segment_ids,\n",
    "            self.masked_lm_positions,\n",
    "            self.masked_lm_ids,\n",
    "            self.masked_lm_weights,\n",
    "            is_training,\n",
    "            config.bert_config)\n",
    "\n",
    "        # prediction\n",
    "        self.masked_lm_predictions = tf.argmax(self.masked_lm_log_probs, axis=-1, output_type=tf.int32)\n",
    "        self.top_n_predictions = tf.nn.top_k(self.probs, k=config.topn, sorted=True, name=\"topn\")\n",
    "\n",
    "    def predict(self, batch, sess):\n",
    "        \"\"\"\n",
    "        for predicting\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids, input_mask, segment_ids, masked_lm_positions, masked_lm_ids, masked_lm_weights = batch\n",
    "\n",
    "        feed_dict = {\n",
    "            self.input_ids: input_ids,\n",
    "            self.input_mask: input_mask,\n",
    "            self.segment_ids: segment_ids,\n",
    "            self.masked_lm_positions: masked_lm_positions,\n",
    "            self.masked_lm_ids: masked_lm_ids,\n",
    "            self.masked_lm_weights: masked_lm_weights\n",
    "        }\n",
    "\n",
    "        masked_lm_predictions, masked_lm_log_probs = sess.run(\n",
    "            [self.masked_lm_predictions, self.masked_lm_log_probs], feed_dict)\n",
    "\n",
    "        return masked_lm_predictions\n",
    "\n",
    "    def topn_predict(self, batch, sess):\n",
    "        \"\"\"\n",
    "        for predicting topn results\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids, input_mask, segment_ids, masked_lm_positions, masked_lm_ids, masked_lm_weights = batch\n",
    "\n",
    "        feed_dict = {\n",
    "            self.input_ids: input_ids,\n",
    "            self.input_mask: input_mask,\n",
    "            self.segment_ids: segment_ids,\n",
    "            self.masked_lm_positions: masked_lm_positions,\n",
    "            self.masked_lm_ids: masked_lm_ids,\n",
    "            self.masked_lm_weights: masked_lm_weights\n",
    "        }\n",
    "\n",
    "        top_n_predictions = sess.run(self.top_n_predictions, feed_dict)\n",
    "        topn_probs, topn_predictions = top_n_predictions\n",
    "\n",
    "        return np.array(topn_probs, dtype=float), topn_predictions\n",
    "\n",
    "    def create_model(self,\n",
    "                     input_ids,\n",
    "                     input_mask,\n",
    "                     segment_ids,\n",
    "                     masked_lm_positions,\n",
    "                     masked_lm_ids,\n",
    "                     masked_lm_weights,\n",
    "                     is_training,\n",
    "                     bert_config):\n",
    "        \"\"\"Create Masked Language Model\"\"\"\n",
    "\n",
    "        model = modeling.BertModel(\n",
    "            config=bert_config,\n",
    "            is_training=is_training,\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            token_type_ids=segment_ids,\n",
    "            use_one_hot_embeddings=False)\n",
    "\n",
    "        masked_lm_loss, masked_lm_example_loss, masked_lm_log_probs, probs = self.get_masked_lm_output(\n",
    "            bert_config, model.get_sequence_output(), model.get_embedding_table(),\n",
    "            masked_lm_positions, masked_lm_ids, masked_lm_weights)\n",
    "\n",
    "        return masked_lm_loss, masked_lm_example_loss, masked_lm_log_probs, probs\n",
    "\n",
    "    @classmethod\n",
    "    def get_masked_lm_output(cls, bert_config, input_tensor, output_weights, positions,\n",
    "                             label_ids, label_weights):\n",
    "        \"\"\"Get loss and log probs for the masked LM.\"\"\"\n",
    "        input_tensor = cls.gather_indexes(input_tensor, positions)\n",
    "\n",
    "        with tf.variable_scope(\"cls/predictions\"):\n",
    "            # We apply one more non-linear transformation before the output layer.\n",
    "            # This matrix is not used after pre-training.\n",
    "            with tf.variable_scope(\"transform\"):\n",
    "                input_tensor = tf.layers.dense(\n",
    "                    input_tensor,\n",
    "                    units=bert_config.hidden_size,\n",
    "                    activation=modeling.get_activation(bert_config.hidden_act),\n",
    "                    kernel_initializer=modeling.create_initializer(\n",
    "                        bert_config.initializer_range))\n",
    "                input_tensor = modeling.layer_norm(input_tensor)\n",
    "\n",
    "            # The output weights are the same as the input embeddings, but there is\n",
    "            # an output-only bias for each token.\n",
    "            output_bias = tf.get_variable(\n",
    "                \"output_bias\",\n",
    "                shape=[bert_config.vocab_size],\n",
    "                initializer=tf.zeros_initializer())\n",
    "            logits = tf.matmul(input_tensor, output_weights, transpose_b=True)\n",
    "            logits = tf.nn.bias_add(logits, output_bias)\n",
    "            log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "            probs = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "            label_ids = tf.reshape(label_ids, [-1])\n",
    "            label_weights = tf.reshape(label_weights, [-1])\n",
    "\n",
    "            one_hot_labels = tf.one_hot(\n",
    "                label_ids, depth=bert_config.vocab_size, dtype=tf.float32)\n",
    "\n",
    "            # The `positions` tensor might be zero-padded (if the sequence is too\n",
    "            # short to have the maximum number of predictions). The `label_weights`\n",
    "            # tensor has a value of 1.0 for every real prediction and 0.0 for the\n",
    "            # padding predictions.\n",
    "            per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[-1])\n",
    "            numerator = tf.reduce_sum(label_weights * per_example_loss)\n",
    "            denominator = tf.reduce_sum(label_weights) + 1e-5\n",
    "            loss = numerator / denominator\n",
    "\n",
    "        return loss, per_example_loss, log_probs, probs\n",
    "\n",
    "    @staticmethod\n",
    "    def gather_indexes(sequence_tensor, positions):\n",
    "        \"\"\"Gathers the vectors at the specific positions over a minibatch.\"\"\"\n",
    "        sequence_shape = modeling.get_shape_list(sequence_tensor, expected_rank=3)\n",
    "        batch_size = sequence_shape[0]\n",
    "        seq_length = sequence_shape[1]\n",
    "        width = sequence_shape[2]\n",
    "\n",
    "        flat_offsets = tf.reshape(\n",
    "            tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])\n",
    "        flat_positions = tf.reshape(positions + flat_offsets, [-1])\n",
    "        flat_sequence_tensor = tf.reshape(sequence_tensor,\n",
    "                                          [batch_size * seq_length, width])\n",
    "        output_tensor = tf.gather(flat_sequence_tensor, flat_positions)\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "class MaskedLM(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        # create session\n",
    "        session_conf = tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=False,\n",
    "            intra_op_parallelism_threads=4,\n",
    "            inter_op_parallelism_threads=4)\n",
    "        session_conf.gpu_options.allow_growth = True\n",
    "        self.session = tf.Session(config=session_conf)\n",
    "\n",
    "        # load model\n",
    "        self.model = self.load_model(config)\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.processor = Processor(config.vocab_file, config.max_seq_length)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(config):\n",
    "\n",
    "        model = Model(config)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "\n",
    "        (assignment_map, initialized_variable_names\n",
    "         ) = modeling.get_assignment_map_from_checkpoint(tvars, config.init_checkpoint)\n",
    "\n",
    "        tf.train.init_from_checkpoint(config.init_checkpoint, assignment_map)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def find_topn_candidates(self, sentences, batch_size=1):\n",
    "        \"\"\"\n",
    "        Args\n",
    "        -----------------------------\n",
    "        sentences: a list of sentences, e.g., ['the man went to the store.', 'he bought a gallon of milk.']\n",
    "        batch_size: default=1\n",
    "\n",
    "        Return\n",
    "        -----------------------------\n",
    "        candidates for each token in the sentences, e.g., [[[('the', 0.88), ('a', 0.65)], ...], [...]]\n",
    "\n",
    "        \"\"\"\n",
    "        data = Data(sentences, self.processor)\n",
    "        stream_res = []\n",
    "        stream_probs = []\n",
    "        lengths = []\n",
    "        while True:\n",
    "            batch = data.next_predict_batch(batch_size)\n",
    "            if batch is not None:\n",
    "                _, id_mask_batch, _, _, _, _ = batch\n",
    "                topn_probs, topn_predictions = self.model.topn_predict(batch, self.session)\n",
    "                lengths.extend(list(np.sum(id_mask_batch, axis=-1)))\n",
    "                stream_res.extend(topn_predictions)\n",
    "                stream_probs.extend(topn_probs)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        res = []\n",
    "        pos = 0\n",
    "        length_id = 0\n",
    "\n",
    "        while pos < len(stream_res):\n",
    "            sen = []\n",
    "            for i in range(self.config.max_seq_length - 2):\n",
    "                if i < lengths[length_id] - 2:  # to account for [CLS] and [SEP]\n",
    "                    token_candidates = []\n",
    "                    for token_idx, prob in zip(stream_res[pos], stream_probs[pos]):\n",
    "                        token_candidates.append((self.processor.idx_to_word[token_idx], prob))\n",
    "                    sen.append(token_candidates)\n",
    "                pos += 1\n",
    "            length_id += 1\n",
    "            res.append(sen)\n",
    "        \n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class Data(object):\n",
    "    \"\"\"\n",
    "    Load data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, processor):\n",
    "\n",
    "        self.data = data\n",
    "        self.pos = 0  # records the iterating progress for df\n",
    "        self.processor = processor\n",
    "\n",
    "    def next_predict_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Produce the next batch for predicting.\n",
    "\n",
    "        Args\n",
    "        ----------------\n",
    "        batch_size: batch_size for predicting\n",
    "\n",
    "        Returns\n",
    "        ----------------\n",
    "        features_padded_batch, tags_padded_batch, length_batch\n",
    "        or\n",
    "        None if the data is exhausted\n",
    "        \"\"\"\n",
    "        print(f'processed {self.pos} entries...')\n",
    "        if self.pos >= len(self.data):\n",
    "            self.pos = 0  # get ready for the next round of prediction\n",
    "\n",
    "            return None\n",
    "\n",
    "        else:\n",
    "            batch = self.data[self.pos: self.pos + batch_size]\n",
    "            self.pos += batch_size\n",
    "\n",
    "            input_ids_batch, \\\n",
    "            input_mask_batch, \\\n",
    "            segment_ids_batch, \\\n",
    "            masked_lm_positions_batch, \\\n",
    "            masked_lm_ids_batch, \\\n",
    "            masked_lm_weights_batch = self.parse(batch)\n",
    "\n",
    "            input_ids_batch = kr.preprocessing.sequence.pad_sequences(input_ids_batch,\n",
    "                                                                      self.processor.max_seq_length,\n",
    "                                                                      padding='post')\n",
    "            input_mask_batch = kr.preprocessing.sequence.pad_sequences(input_mask_batch,\n",
    "                                                                       self.processor.max_seq_length,\n",
    "                                                                       padding='post')\n",
    "            segment_ids_batch = kr.preprocessing.sequence.pad_sequences(segment_ids_batch,\n",
    "                                                                        self.processor.max_seq_length,\n",
    "                                                                        padding='post')\n",
    "\n",
    "            masked_lm_positions_batch = kr.preprocessing.sequence.pad_sequences(masked_lm_positions_batch,\n",
    "                                                                                self.processor.max_seq_length - 2,\n",
    "                                                                                padding='post')\n",
    "            masked_lm_ids_batch = kr.preprocessing.sequence.pad_sequences(masked_lm_ids_batch,\n",
    "                                                                          self.processor.max_seq_length - 2,\n",
    "                                                                          padding='post')\n",
    "            masked_lm_weights_batch = kr.preprocessing.sequence.pad_sequences(masked_lm_weights_batch,\n",
    "                                                                              self.processor.max_seq_length - 2,\n",
    "                                                                              padding='post')\n",
    "\n",
    "            return input_ids_batch, input_mask_batch, segment_ids_batch, masked_lm_positions_batch, masked_lm_ids_batch, masked_lm_weights_batch\n",
    "\n",
    "    def parse(self, batch):\n",
    "        input_ids_batch, \\\n",
    "        input_mask_batch, \\\n",
    "        segment_ids_batch, \\\n",
    "        masked_lm_positions_batch, \\\n",
    "        masked_lm_ids_batch, \\\n",
    "        masked_lm_weights_batch = [], [], [], [], [], []\n",
    "        for sentence in batch:\n",
    "\n",
    "            input_ids, input_mask, segment_ids, masked_lm_positions, masked_lm_ids, masked_lm_weights = \\\n",
    "                self.processor.create_single_instance(sentence)\n",
    "            input_ids_batch.append(input_ids)\n",
    "            input_mask_batch.append(input_mask)\n",
    "            segment_ids_batch.append(segment_ids)\n",
    "            masked_lm_positions_batch.append(masked_lm_positions)\n",
    "            masked_lm_ids_batch.append(masked_lm_ids)\n",
    "            masked_lm_weights_batch.append(masked_lm_weights)\n",
    "\n",
    "        return input_ids_batch, input_mask_batch, segment_ids_batch, masked_lm_positions_batch, masked_lm_ids_batch, masked_lm_weights_batch\n",
    "\n",
    "\n",
    "class Processor(object):\n",
    "    def __init__(self, vocab_file, max_seq_length):\n",
    "        self.tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file)\n",
    "        self.idx_to_word = self.inverse_vocab(self.tokenizer.vocab)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    @staticmethod\n",
    "    def inverse_vocab(vocab):\n",
    "        idx_to_word = {}\n",
    "        for word in vocab:\n",
    "            idx_to_word[vocab[word]] = word\n",
    "        return idx_to_word\n",
    "\n",
    "    def create_single_instance(self, sentence):\n",
    "        # tokenization\n",
    "        tokens_raw = self.tokenizer.tokenize(tokenization.convert_to_unicode(sentence))\n",
    "\n",
    "        # add [CLS] and [SEP]\n",
    "        assert len(sentence) <= self.max_seq_length - 2\n",
    "        tokens = [\"[CLS]\"] + tokens_raw + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        # produce pseudo ground truth, since the truth is unknown when it comes to spelling checking.\n",
    "        input_tokens, masked_lm_positions, masked_lm_labels = self.create_pseudo_ground_truth(tokens)\n",
    "\n",
    "        # convert to ids\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "        segment_ids = list(segment_ids)\n",
    "\n",
    "        masked_lm_positions = list(masked_lm_positions)\n",
    "        masked_lm_ids = self.tokenizer.convert_tokens_to_ids(masked_lm_labels)\n",
    "        masked_lm_weights = [1.0] * len(masked_lm_ids)\n",
    "\n",
    "        # print(input_tokens)\n",
    "\n",
    "        return input_ids, input_mask, segment_ids, masked_lm_positions, masked_lm_ids, masked_lm_weights\n",
    "\n",
    "    @staticmethod\n",
    "    def create_pseudo_ground_truth(tokens):\n",
    "        input_tokens = list(tokens)\n",
    "        masked_lm_positions = []\n",
    "        masked_lm_labels = []\n",
    "\n",
    "        for index, token in enumerate(tokens):\n",
    "\n",
    "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "                continue\n",
    "\n",
    "            masked_token = tokens[index]  # keep the original token\n",
    "\n",
    "            input_tokens[index] = masked_token\n",
    "            masked_lm_positions.append(index)\n",
    "            masked_lm_labels.append(tokens[index])\n",
    "\n",
    "        return input_tokens, masked_lm_positions, masked_lm_labels\n",
    "\n",
    "\n",
    "\n",
    "def test_masked_lm():\n",
    "    config = Config()\n",
    "    lm = MaskedLM(config)\n",
    "    res = lm.find_topn_candidates(\n",
    "         ['生成形近字的候选词的测试文字'], 2)\n",
    "    return res\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 entries...\n",
      "processed 2 entries...\n"
     ]
    }
   ],
   "source": [
    "test = test_masked_lm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 70 into shape (7,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b86072ee1a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 70 into shape (7,5)"
     ]
    }
   ],
   "source": [
    "data = np.array(test)\n",
    "data[:,:,:,0].flatten().reshape(7,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data[:,:,:,0].flatten().reshape(7,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "lm = MaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
